{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"  # makes some CUDA calls deterministic\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List, Iterator\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as vF\n",
    "from tqdm.notebook import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def coords_tensor(D, n):\n",
    "  coord_vals = torch.arange(n) / (n-1)\n",
    "  coord_components = D*[coord_vals]\n",
    "  X = torch.stack(torch.meshgrid(coord_components, indexing='ij'))\n",
    "  s = list(range(1, D+1)) + [0]  # result is like [1,2,3,0]\n",
    "  X = X.permute(s).flatten(0, D-1)\n",
    "  return X\n",
    "# print(coords_tensor(2, 3))\n",
    "\n",
    "def pil_to_samples(img):\n",
    "  D = 1 if (img.height == 1) else 2\n",
    "  n = img.width\n",
    "  t = vF.to_tensor(img).squeeze().cuda()\n",
    "  Y = t.flatten().unsqueeze(1)\n",
    "  X = coords_tensor(D, n)\n",
    "  return X, Y\n",
    "\n",
    "def samples_to_pil(X, Y):\n",
    "  D = X.shape[1]\n",
    "  n = round(len(Y)**(1/D))\n",
    "  t = Y.reshape([1] + D*[n])\n",
    "  return vF.to_pil_image(t)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class Config(NamedTuple):\n",
    "  # signal\n",
    "  D: int  # number of dimensions\n",
    "  sig: str  # signal/image file name, minus '.png'\n",
    "  # encoding\n",
    "  type: str  # see complex_ef() for options\n",
    "  gen: str  # see gen_fn_for(name) for options\n",
    "  N: int\n",
    "  # architecture\n",
    "  H: int = 0\n",
    "  G: int = 1\n",
    "  # init and training\n",
    "  seed: int = 0\n",
    "  opt: str = 'tpc'\n",
    "  mlr: int = 1  # milli learning rate  # DEPRECATED, unused\n",
    "  epoch_count: int = 10000\n",
    "\n",
    "  @classmethod\n",
    "  def from_path(cls, path):\n",
    "    d = torch.load(path)\n",
    "    return Config(**d)\n",
    "\n",
    "  def replace(self, **kwargs):\n",
    "    return self._replace(**kwargs)\n",
    "\n",
    "  def compression_ratio(self):\n",
    "    # X, Y = pil_to_samples(PIL.Image.open(self.sig_path))\n",
    "    # uncompressed: int = len(Y)\n",
    "    # TODO: don't hardcode these resolutions\n",
    "    uncompressed = 1260**2\n",
    "    if self.D == 1:\n",
    "      uncompressed = 2040\n",
    "    if self.sig[0] == 'B':\n",
    "      uncompressed = 1023**2\n",
    "    if self.sig[0] == 's':\n",
    "      uncompressed = 512**2\n",
    "    compressed: int = self.P\n",
    "    return compressed / uncompressed\n",
    "\n",
    "  @property\n",
    "  def P(self) -> int:\n",
    "    if self.H == 0:\n",
    "      return self.NN_input_width\n",
    "    else:\n",
    "      return (self.NN_input_width * self.G) + (self.H - 1) * (self.G * self.G) + (self.G * 1)\n",
    "\n",
    "  def count_params(self, trainer_to_check=None):\n",
    "    P = self.P\n",
    "    if trainer_to_check is not None:\n",
    "      actual = sum(p.numel() for p in trainer_to_check.net.parameters() if p.requires_grad)\n",
    "      assert P == actual\n",
    "    widths: List[int] = self.NN_layer_widths\n",
    "    total_by_widths = 0\n",
    "    for i in range(1, len(widths)):\n",
    "      total_by_widths += widths[i-1] * widths[i]\n",
    "    assert P == total_by_widths\n",
    "    return P\n",
    "\n",
    "  @property\n",
    "  def sig_path(self):\n",
    "    return f'../../exps/{self.D}d/{self.sig}.png'\n",
    "\n",
    "  @property\n",
    "  def dir_path(self):\n",
    "    return '/'.join([\n",
    "      f'../../exps/results',\n",
    "      f'D_{self.D}',\n",
    "      f'sig_{self.sig}',\n",
    "      f'type_{self.type} gen_{self.gen} N_{self.N}',\n",
    "      f'H_{self.H} G_{self.G}',\n",
    "      f'opt_{self.opt} mlr_{self.mlr} seed_{self.seed}'\n",
    "    ])\n",
    "\n",
    "  @property\n",
    "  def NN_input_width(self) -> int:\n",
    "    if self.type == 'simple':\n",
    "      return self.N * self.D\n",
    "    else:\n",
    "      return self.N**self.D\n",
    "\n",
    "  @property\n",
    "  def NN_layer_widths(self) -> List[int]:\n",
    "    return [self.NN_input_width] + self.H * [self.G] + [1]\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self.mlr / 1000\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def gen_fn_for(name):\n",
    "  return {\n",
    "    'ident': lambda x: x,\n",
    "    'rect' : lambda x: 0.5 * (torch.sign(x+0.5) - torch.sign(x-0.5)),\n",
    "    'tri'  : lambda x: torch.max(1 - torch.abs(x), torch.zeros_like(x)),\n",
    "    'gauss': lambda x: torch.exp(-0.5 * x*x),\n",
    "    'sinc' : lambda x: torch.sinc(x),\n",
    "  }[name]\n",
    "\n",
    "def simple_ef(x, gen_fn, N=1):\n",
    "  encoded = torch.zeros((x.shape[0], x.shape[1] * N))\n",
    "  offsets = torch.arange(0, N).unsqueeze(0)\n",
    "  for i in range(x.shape[1]):\n",
    "    encoded[:, i * N:(i + 1) * N] = gen_fn(x[:, i].unsqueeze(1) * (N - 1) - offsets)\n",
    "  return encoded\n",
    "\n",
    "def complex_ef(x, gen_fn, N=1):\n",
    "  offsets = torch.arange(0, N).unsqueeze(0)\n",
    "  enc0 = gen_fn(x[:, 0].unsqueeze(1) * (N - 1) - offsets).unsqueeze(1)\n",
    "  enc1 = gen_fn(x[:, 1].unsqueeze(1) * (N - 1) - offsets).unsqueeze(2)\n",
    "  encoded = torch.mul(enc0, enc1).flatten(1)\n",
    "  return encoded\n",
    "\n",
    "def fixargs(fn, **fixedkwargs):\n",
    "  def newfn(*args, **kwargs):\n",
    "    newkwargs = dict(fixedkwargs)\n",
    "    newkwargs.update(kwargs)\n",
    "    return fn(*args, **newkwargs)\n",
    "  return newfn\n",
    "\n",
    "def make_encoder(config: Config):\n",
    "  gen_fn = gen_fn_for(config.gen)\n",
    "  type_fn = {\n",
    "    'simple': simple_ef,\n",
    "    'complex': complex_ef,\n",
    "  }[config.type]\n",
    "  enc = fixargs(\n",
    "    type_fn,\n",
    "    gen_fn=gen_fn,\n",
    "    N=config.N)\n",
    "  return enc\n",
    "\n",
    "\n",
    "def plot_enc():\n",
    "  cfg = Config(D=2, sig='', type='complex', gen='tri', N=3, H=0, G=0)\n",
    "  enc = make_encoder(cfg)\n",
    "  n = 41\n",
    "  coords = coords_tensor(2, n)\n",
    "  print(coords.shape)\n",
    "  pic = enc(coords)\n",
    "  print(pic.shape)\n",
    "  fig, ax = plt.subplots(ncols=1)\n",
    "  ax.imshow(pic[:, 0].reshape([n,n]).cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "# plot_enc()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class AdamOpt(object):\n",
    "  def __init__(self, net, phix, y):\n",
    "    self.net = net\n",
    "    self.PhiX = phix\n",
    "    self.Y = y\n",
    "    self.criterion = nn.MSELoss()\n",
    "    self.optimizer = torch.optim.Adam(self.net.parameters())  #, lr=<DEFAULT>=1e-3)\n",
    "\n",
    "  def load_state_dict(self, t):\n",
    "    self.optimizer.load_state_dict(t)\n",
    "\n",
    "  def state_dict(self):\n",
    "    return self.optimizer.state_dict()\n",
    "\n",
    "  def one_epoch(self):\n",
    "    self.optimizer.zero_grad()\n",
    "    loss = self.criterion(self.net(self.PhiX), self.Y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "class TpcAdamOpt(object):\n",
    "  # Adam, with parameters copied from the TPC paper demo code:\n",
    "  # https://colab.research.google.com/github/osiriszjq/complex_encoding/blob/main/complex_encoding.ipynb\n",
    "  def __init__(self, net, phix, y):\n",
    "    self.net = net\n",
    "    self.PhiX = phix\n",
    "    self.Y = y\n",
    "    self.criterion = nn.MSELoss()\n",
    "    self.optimizer = torch.optim.Adam(self.net.parameters(), betas=(0.9, 0.999), weight_decay=1e-8)\n",
    "\n",
    "  def load_state_dict(self, t):\n",
    "    self.optimizer.load_state_dict(t)\n",
    "\n",
    "  def state_dict(self):\n",
    "    return self.optimizer.state_dict()\n",
    "\n",
    "  def one_epoch(self):\n",
    "    self.optimizer.zero_grad()\n",
    "    loss = self.criterion(self.net(self.PhiX), self.Y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def opt_method_by_name(name: str):\n",
    "  return {\n",
    "    'adam': AdamOpt,\n",
    "    'tpc': TpcAdamOpt,\n",
    "  }[name]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def layers_for(widths: List[int]) -> Iterator[nn.Module]:\n",
    "  for i in range(len(widths)-1):\n",
    "    if i:\n",
    "      yield nn.ReLU()\n",
    "    yield nn.Linear(widths[i], widths[i+1], bias=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class Trainer(object):\n",
    "  epoch: int\n",
    "  loss_series: torch.Tensor\n",
    "  time_series: torch.Tensor\n",
    "  net: torch.nn.Module\n",
    "\n",
    "  def __init__(self, config: Config, saveload: bool = True):\n",
    "    if config.compression_ratio() > 1:\n",
    "      print(f'Warning: Overlarge model {config.dir_path}')\n",
    "    self.cfg = config\n",
    "    self.saveload = saveload\n",
    "    # common init\n",
    "    self.img = PIL.Image.open(self.cfg.sig_path)\n",
    "    self.X, self.Y = pil_to_samples(self.img)\n",
    "    self.Phi = make_encoder(self.cfg)\n",
    "    self.PhiX = self.Phi(self.X)\n",
    "    # epoch 0 init\n",
    "    torch.manual_seed(self.cfg.seed)\n",
    "    np.random.seed(self.cfg.seed)\n",
    "    self.epoch = 0\n",
    "    self.summary = {}\n",
    "    self.loss_series = torch.zeros((500 + 1,))\n",
    "    self.time_series = torch.zeros((500,))\n",
    "    self.net = nn.Sequential(*layers_for(self.cfg.NN_layer_widths))\n",
    "    self.opt = opt_method_by_name(self.cfg.opt)(self.net, self.PhiX, self.Y)\n",
    "    # load latest epoch if possible\n",
    "    self.load()\n",
    "\n",
    "  def load(self):\n",
    "    if not self.saveload:\n",
    "      return\n",
    "    try:\n",
    "      summa = torch.load(f'{self.cfg.dir_path}/summary.pt')\n",
    "      losss = torch.load(f'{self.cfg.dir_path}/loss_series.pt')\n",
    "      times = torch.load(f'{self.cfg.dir_path}/time_series.pt')\n",
    "      netsd = torch.load(f'{self.cfg.dir_path}/net_state.pt')\n",
    "      optsd = torch.load(f'{self.cfg.dir_path}/opt_state.pt')\n",
    "    except OSError:\n",
    "      return\n",
    "    self.summary = summa\n",
    "    self.loss_series = losss\n",
    "    self.time_series = times\n",
    "    self.epoch = len(self.time_series)\n",
    "    self.net.load_state_dict(netsd)\n",
    "    self.opt.load_state_dict(optsd)\n",
    "    self.net.train()\n",
    "\n",
    "  def update_summary(self):\n",
    "    self.summary['P'] = self.cfg.count_params(self)\n",
    "\n",
    "  def save(self):\n",
    "    if not self.saveload:\n",
    "      return\n",
    "    os.makedirs(self.cfg.dir_path, exist_ok=True)\n",
    "    self.update_summary()\n",
    "    torch.save(self.cfg._asdict(),           f'{self.cfg.dir_path}/config.pt')\n",
    "    torch.save(self.summary,                 f'{self.cfg.dir_path}/summary.pt')\n",
    "    torch.save(self.loss_series,             f'{self.cfg.dir_path}/loss_series.pt')\n",
    "    torch.save(self.time_series,             f'{self.cfg.dir_path}/time_series.pt')\n",
    "    torch.save(self.net.state_dict(),        f'{self.cfg.dir_path}/net_state.pt')\n",
    "    torch.save(self.opt.state_dict(),        f'{self.cfg.dir_path}/opt_state.pt')\n",
    "\n",
    "  def Yhat(self):\n",
    "    return self.net(self.PhiX)\n",
    "\n",
    "  def loss(self):\n",
    "    return nn.functional.mse_loss(self.Yhat(), self.Y).item()\n",
    "\n",
    "  def ensure_space(self):\n",
    "    if self.epoch >= len(self.time_series):\n",
    "      extra = len(self.loss_series)\n",
    "      self.loss_series = torch.cat([self.loss_series, torch.zeros((extra,))])\n",
    "      self.time_series = torch.cat([self.time_series, torch.zeros((extra,))])\n",
    "\n",
    "  def train(self, stopper) -> bool:  # returns whether progress was made\n",
    "    if stopper.stopnow():\n",
    "      return False\n",
    "    self.net.train()\n",
    "    while not stopper.stopnow():\n",
    "      start = time.time_ns()\n",
    "      loss = self.opt.one_epoch()\n",
    "      self.ensure_space()\n",
    "      self.loss_series[self.epoch] = loss\n",
    "      self.time_series[self.epoch] = time.time_ns() - start\n",
    "      self.epoch += 1\n",
    "    self.ensure_space()\n",
    "    self.loss_series[self.epoch] = self.loss()\n",
    "    self.loss_series = self.loss_series[:self.epoch + 1]\n",
    "    self.time_series = self.time_series[:self.epoch]\n",
    "    self.save()  # TODO: save periodically\n",
    "    return True\n",
    "\n",
    "\n",
    "class Stopper(object):\n",
    "  def __init__(self, trainer: Trainer, epoch: int = None, dtime: float = None, max_progratio: float = None):\n",
    "    self.tr = trainer\n",
    "    self.stopepoch = epoch or 1000000000\n",
    "    self.dtime = dtime\n",
    "    self.max_progratio = max_progratio\n",
    "    self.stoptime = None\n",
    "\n",
    "  def start(self) -> bool:  # returns whether progress was made\n",
    "    if self.dtime:\n",
    "      self.stoptime = time.time_ns() + self.dtime * 1e9\n",
    "    return self.tr.train(self)\n",
    "\n",
    "  def stuck(self):\n",
    "    return self.tr.epoch >= 1000 and float(self.tr.loss_series[1000]) > 0.3\n",
    "\n",
    "  def progratio(self):\n",
    "    if self.tr.epoch < 1000:\n",
    "      return 0.0\n",
    "    l_half = float(self.tr.loss_series[self.tr.epoch//2])\n",
    "    l_now = float(self.tr.loss_series[self.tr.epoch])\n",
    "    if l_half == 0:\n",
    "      return 1.0\n",
    "    return l_now / l_half\n",
    "\n",
    "  def stopnow(self):\n",
    "    if self.stuck():\n",
    "      return True\n",
    "    if self.max_progratio and self.progratio() >= self.max_progratio:\n",
    "      return True\n",
    "    if self.stopepoch and self.tr.epoch >= self.stopepoch:\n",
    "      return True\n",
    "    if self.stoptime and time.time_ns() > self.stoptime:\n",
    "      return True\n",
    "    return False\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# def run_many():\n",
    "#   configs = []\n",
    "#   for N in range(2, 12+1):\n",
    "#     sizes = []\n",
    "#     comp = Config(D=2, sig='', type='complex', gen='sinc', N=N, H=0, G=1)\n",
    "#     simp = Config(D=2, sig='', type='simple', gen='sinc', N=N, H=1, G=1)\n",
    "#     for H in [1, 2, 4, 8]:\n",
    "#       hsimp = simp.replace(H=H)\n",
    "#       best = None\n",
    "#       for G in range(2, 1000+1):\n",
    "#         gsimp = hsimp.replace(G=G)\n",
    "#         if gsimp.P > comp.P:\n",
    "#           break\n",
    "#         best = gsimp\n",
    "#       if best is None:\n",
    "#         break\n",
    "#       configs.append(best)\n",
    "#       sizes.append(best.G)\n",
    "#     print(N, sizes)\n",
    "#\n",
    "#   configs = [c for c in configs if c.compression_ratio() <= 1]  # remove overlarge models\n",
    "#   configs.sort(key=lambda x: x.P, reverse=True)\n",
    "#   # for c in configs:\n",
    "#   #   print(c.dir_path, c.P)\n",
    "#\n",
    "#   for newseed in range(1000):\n",
    "#     for config in tqdm(configs):\n",
    "#       # for fname in ['0801_penguin', '0809_lion', '0823_greece', '0872_walnuts', '0887_castle']:\n",
    "#       config = config.replace(seed=newseed, sig='0823_greece')\n",
    "#       trainer = Trainer(config)\n",
    "#       fin = trainer.is_finished\n",
    "#       if fin:\n",
    "#         print(f'{config.dir_path} already complete')\n",
    "#       else:\n",
    "#         trainer.train()\n",
    "#         print(f'{config.P}\\t{trainer.loss_series[-1].item()}\\t{config.dir_path}')\n",
    "\n",
    "\n",
    "def run_comparable_N_P():\n",
    "  configs = []\n",
    "  for N in [8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256, 384]:\n",
    "    comp = Config(D=2, sig='s0823_greece', type='complex', gen='sinc', N=N, H=0, G=1)\n",
    "    for H in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "      most_P = None\n",
    "      for G in range(1, 2*N):\n",
    "        c = comp.replace(type='simple', H=H, G=G)\n",
    "        if c.P > comp.P:\n",
    "          break\n",
    "        most_P = c\n",
    "      if most_P:\n",
    "        configs.append(most_P)\n",
    "\n",
    "  # configs.reverse()\n",
    "  # configs.sort(key=lambda x: x.P, reverse=True)\n",
    "  # for c in configs:\n",
    "  #   print(c.dir_path, c.P)\n",
    "\n",
    "  seed_by_cfg = defaultdict(int)\n",
    "  for trial in range(1, 3+1):\n",
    "    print(f'Starting trial #{trial}')\n",
    "    for config in configs:\n",
    "      while True:\n",
    "        c = config.replace(seed=seed_by_cfg[config])\n",
    "        stopper = Stopper(Trainer(c), epoch=1000)\n",
    "        stopper.start()\n",
    "        stuck = stopper.stuck()\n",
    "        del stopper\n",
    "        if not stuck:\n",
    "          break\n",
    "        print(f'Skipping bad seed: {c}')\n",
    "        seed_by_cfg[config] += 1\n",
    "    for req_progratio in [0.5, 0.6, 0.7, 0.8, 0.86, 0.9, 0.93, 0.96]:\n",
    "      print(f'Progratio: {req_progratio}')\n",
    "      progcount = 1\n",
    "      prevprogcount = -1\n",
    "      while progcount:\n",
    "        progcount = 0\n",
    "        for config in configs:\n",
    "          stopper = Stopper(\n",
    "            Trainer(config.replace(seed=seed_by_cfg[config])),\n",
    "            dtime=60.0,\n",
    "            max_progratio=req_progratio\n",
    "          )\n",
    "          prog = stopper.start()\n",
    "          progcount += int(prog)\n",
    "          del stopper\n",
    "        if progcount != prevprogcount:\n",
    "          prevprogcount = progcount\n",
    "          print(f'Number of models making progress: {progcount}/{len(configs)}')\n",
    "    print(f'Trial #{trial} finished. Incrementing seeds.')\n",
    "    for config in configs:\n",
    "      seed_by_cfg[config] += 1\n",
    "\n",
    "\n",
    "run_comparable_N_P()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def run_1D():\n",
    "  configs = []\n",
    "  cf = Config(D=1, sig='0823_greece', type='simple', gen='sinc', N=1000, H=0, G=1)\n",
    "  periodN = 10\n",
    "  for N in range(periodN, 100+1, periodN):\n",
    "    for H in range(1, 10+1):\n",
    "      most_P = None\n",
    "      for G in range(2, 2040):\n",
    "        c = cf.replace(type='simple', N=N, H=H, G=G)\n",
    "        if c.P > cf.P:\n",
    "          break\n",
    "        most_P = c\n",
    "      if most_P:\n",
    "        configs.append(most_P)\n",
    "\n",
    "  # configs.reverse()\n",
    "\n",
    "  configs = [c for c in configs if c.compression_ratio() <= 1]  # remove overlarge models\n",
    "  # configs = [c for c in configs if c.P <= 2000]  # remove overlarge models\n",
    "  configs.sort(key=lambda x: x.P, reverse=True)\n",
    "  # for c in configs:\n",
    "  #   print(c.dir_path, c.P)\n",
    "\n",
    "  for newseed in range(1000):#, desc='seeds'):\n",
    "    for config in tqdm(configs, desc='configs', leave=True):\n",
    "      trainer = Trainer(config.replace(seed=newseed))\n",
    "      if trainer.is_finished:\n",
    "        continue\n",
    "      trainer.train()\n",
    "      # print(f'{config.dir_path}\\t{trainer.loss_series[-1].item()}\\t{config.P}')\n",
    "\n",
    "\n",
    "# run_1D()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def find_prop(rootdir: str = '../../exps/results'):\n",
    "  cfgs = []\n",
    "  for dirpath, dirnames, filenames in os.walk(rootdir):\n",
    "    for filename in filenames:\n",
    "      if filename == 'config.pt':\n",
    "        cfgs.append(Config.from_path(os.path.join(dirpath, filename)))\n",
    "  rcfgs = []\n",
    "  for c in tqdm(cfgs):\n",
    "    r = c.compression_ratio()\n",
    "    if r >= 1:\n",
    "      rcfgs.append((c.dir_path, r))\n",
    "  rcfgs.sort(reverse=True)\n",
    "  for p, r in rcfgs:\n",
    "    print(p, r)\n",
    "\n",
    "# find_prop()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def resave_path(path: str, verbose: bool = True):\n",
    "  if verbose:\n",
    "    print(f'resaving path: {path[:-9]} ...')\n",
    "  cfg = Config.from_path(path)\n",
    "  prevrun = Trainer(cfg)\n",
    "  prevrun.save()\n",
    "\n",
    "def resave_all(rootdir: str = '../../exps/results'):\n",
    "  paths = []\n",
    "  for dirpath, dirnames, filenames in os.walk(rootdir):\n",
    "    for filename in filenames:\n",
    "      if filename == 'config.pt':\n",
    "        paths.append(os.path.join(dirpath, filename))\n",
    "  for path in tqdm(paths):\n",
    "    resave_path(path, verbose=False)\n",
    "  print(f'resaved {len(paths)} runs')\n",
    "\n",
    "# resave_path('/home/alex/hp/exps/results/D_1/sig_0801_penguin/type_simple gen_sinc N_2/H_0 G_1/opt_adam mlr_1 seed_0/config.pt')\n",
    "# resave_all()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def verify_path(path: str, epochs: int = 1, verbose: bool = True):\n",
    "  if verbose:\n",
    "    print(f'checking path: {path}')\n",
    "  cfg = Config.from_path(path)\n",
    "  rerun = Trainer(cfg.replace(epoch_count=epochs), saveload=False)\n",
    "  rerun.train()\n",
    "  rerun_loss_series = rerun.loss_series[:epochs+1].detach()\n",
    "  del rerun\n",
    "  prevrun = Trainer(cfg)\n",
    "  # simple checks\n",
    "  assert len(prevrun.loss_series) == cfg.epoch_count + 1\n",
    "  assert len(prevrun.time_series) == cfg.epoch_count\n",
    "  # rerun some training\n",
    "  assert torch.equal(prevrun.loss_series[:epochs+1], rerun_loss_series[:epochs+1])\n",
    "  del prevrun\n",
    "  if verbose:\n",
    "    print(f'all checks out')\n",
    "\n",
    "def verify_all(rootdir: str = '../../exps/results'):\n",
    "  paths = []\n",
    "  for dirpath, dirnames, filenames in os.walk(rootdir):\n",
    "    if dirpath.count('lstsq'):\n",
    "      continue\n",
    "    for filename in filenames:\n",
    "      if filename == 'config.pt':\n",
    "        paths.append(os.path.join(dirpath, filename))\n",
    "  random.shuffle(paths)  # so that there is no run that is always checked last\n",
    "  for path in tqdm(paths):\n",
    "    verify_path(path, verbose=False)\n",
    "  print(f'verified {len(paths)} runs')\n",
    "\n",
    "# verify_path('/home/alex/hp/exps/results/D_1/sig_0801_penguin/type_simple gen_sinc N_2/H_0 G_1/opt_adam mlr_1 seed_0/config.pt', epochs=10)\n",
    "# verify_all()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
